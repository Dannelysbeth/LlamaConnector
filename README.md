# Local LLaMA 3.2 â€“ Ollama Example

Minimal Python example showing how to call a **local LLaMA 3.2 model** running via **Ollama** using its HTTP API.

## Prerequisites
- Ollama running locally
- Model pulled:
  ```bash
  ollama pull llama3.2
